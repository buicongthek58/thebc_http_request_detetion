{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "from colorama import Fore\n",
    "from sklearn.metrics import auc, roc_curve, precision_score, recall_score\n",
    "\n",
    "from utils.vocab import Vocabulary\n",
    "from utils.reader import Data\n",
    "from utils.utils import print_progress, create_checkpoints_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main model parameters:\n",
    "- *batch_size* - the number of samples in a batch\n",
    "- *embed_size* - the dimension of embedding space (should be less than vocabulary size)\n",
    "- *hidden_size* - the number of hidden states in lstm \n",
    "- *num_layers* - the number of lstm blocks\n",
    "- *checkpoints* - path to checkpoint directory\n",
    "- *std_factor* - the number of stds that is used for defining a model threshold\n",
    "- *dropout* - the probability that each element is kept\n",
    "- *vocab* - the Vocabulary object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 21991 samples\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"batch_size\": 128,\n",
    "    \"embed_size\": 64,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"checkpoints\": \"./checkpoints/\",\n",
    "    \"std_factor\": 6.,\n",
    "    \"dropout\": 0.7,\n",
    "}\n",
    "\n",
    "path_normal_data = \"datasets/vulnbank_train.txt\"\n",
    "path_anomaly_data = \"datasets/vulnbank_anomaly.txt\"\n",
    "\n",
    "create_checkpoints_dir(params[\"checkpoints\"])\n",
    "\n",
    "vocab = Vocabulary()\n",
    "params[\"vocab\"] = vocab\n",
    "\n",
    "d = Data(path_normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code the Sequence-to-Sequence model for determining anomalies is defined.  \n",
    "The same sequences are fed to the input and output of the model. So the model learns to reconstruct them. At the stage of training and validation, only valid samples are submitted to the model. The validation phase is needed in order to initialize the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq():\n",
    "    def __init__(self, args):\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "        self.max_seq_len = tf.placeholder(tf.int32, [], name='max_seq_len')\n",
    "        self.inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "        self.lengths = tf.placeholder(tf.int32, [None, ], name='lengths')\n",
    "        self.dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "        \n",
    "        self.num_layers = args['num_layers']\n",
    "        self.hidden_size = args['hidden_size']\n",
    "        self.vocab = args['vocab']\n",
    "\n",
    "        dec_input = self._process_decoder_input(\n",
    "            self.targets,\n",
    "            self.vocab.vocab,\n",
    "            tf.to_int32(self.batch_size))\n",
    "\n",
    "        vocab_size = len(self.vocab.vocab)\n",
    "\n",
    "        # Embeddings for inputs\n",
    "        embed_initializer = tf.random_uniform_initializer(-np.sqrt(3), np.sqrt(3))\n",
    "\n",
    "        with tf.variable_scope('embedding'):\n",
    "            embeds = tf.get_variable(\n",
    "                'embed_matrix',\n",
    "                [vocab_size, args['embed_size']],\n",
    "                initializer=embed_initializer,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            enc_embed_input = tf.nn.embedding_lookup(embeds, self.inputs)\n",
    "            \n",
    "        enc_state = self._encoder(enc_embed_input)\n",
    "        \n",
    "        # Embeddings for outputs\n",
    "        with tf.variable_scope('embedding', reuse=True):\n",
    "            dec_embed_input = tf.nn.embedding_lookup(embeds, dec_input)\n",
    "\n",
    "        dec_outputs = self._decoder(enc_state, dec_embed_input)\n",
    "\n",
    "        weight, bias = self._weight_and_bias(args['hidden_size'], vocab_size)\n",
    "        outputs = tf.reshape(dec_outputs[0].rnn_output, [-1, args['hidden_size']])\n",
    "        logits = tf.matmul(outputs, weight) + bias\n",
    "\n",
    "        logits = tf.reshape(logits, [-1, self.max_seq_len, vocab_size], name='logits')\n",
    "        self.probs = tf.nn.softmax(logits, name='probs')\n",
    "        self.decoder_outputs = tf.argmax(logits, axis=2)\n",
    "\n",
    "        self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits,\n",
    "            labels=self.targets,\n",
    "            name='cross_entropy')\n",
    "        self.batch_loss = tf.identity(tf.reduce_mean(self.cross_entropy, axis=1), name='batch_loss')\n",
    "        self.loss = tf.reduce_mean(self.cross_entropy)\n",
    "\n",
    "        self.train_optimizer = self._optimizer(self.loss)\n",
    "\n",
    "        # Saver\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def _encoder(self, enc_embed_input):\n",
    "        \"\"\"\n",
    "        Adds an encoder to the model architecture.\n",
    "        \"\"\"\n",
    "        cells = [self._lstm_cell(self.hidden_size) for _ in range(self.num_layers)]\n",
    "        multilstm = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        _, enc_state = tf.nn.dynamic_rnn(\n",
    "            multilstm,\n",
    "            enc_embed_input,\n",
    "            sequence_length=self.lengths,\n",
    "            swap_memory=True,\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "        return enc_state\n",
    "    \n",
    "    def _decoder(self, enc_state, dec_embed_input):\n",
    "        \"\"\"\n",
    "        Adds a decoder to the model architecture.\n",
    "        \"\"\"\n",
    "        output_lengths = tf.ones([self.batch_size], tf.int32) * self.max_seq_len\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            dec_embed_input,\n",
    "            output_lengths,\n",
    "            time_major=False)\n",
    "\n",
    "        cells = [self._lstm_cell(self.hidden_size) for _ in range(self.num_layers)]\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, enc_state)\n",
    "\n",
    "        dec_outputs = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=self.max_seq_len, swap_memory=True)\n",
    "        \n",
    "        return dec_outputs\n",
    "    \n",
    "    def _optimizer(self, loss,):\n",
    "        \"\"\"\n",
    "        Optimizes weights given a loss. \n",
    "        \"\"\"\n",
    "        def _learning_rate_decay_fn(learning_rate, global_step):\n",
    "            return tf.train.exponential_decay(learning_rate, global_step, decay_steps=10000, decay_rate=0.99)\n",
    "\n",
    "        starting_lr = 0.001\n",
    "        starting_global_step = tf.Variable(0, trainable=False)\n",
    "        optimizer = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=starting_global_step,\n",
    "            learning_rate=starting_lr,\n",
    "            optimizer=tf.train.AdamOptimizer,\n",
    "            learning_rate_decay_fn=lambda lr, gs: _learning_rate_decay_fn(lr, gs),\n",
    "            clip_gradients=5.0)\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def _process_decoder_input(self, target_data, char_to_code, batch_size):\n",
    "        \"\"\"\n",
    "        Concatenates the <GO> to the begining of each batch.\n",
    "        \"\"\"\n",
    "        ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "        dec_input = tf.concat([tf.fill([batch_size, 1], char_to_code['<GO>']), ending], 1)\n",
    "\n",
    "        return dec_input\n",
    "\n",
    "    def _lstm_cell(self, hidden_size):\n",
    "        \"\"\"\n",
    "        Returns LSTM cell with dropout.\n",
    "        \"\"\"\n",
    "        cell = tf.contrib.rnn.LSTMCell(\n",
    "            hidden_size,\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.dropout)\n",
    "\n",
    "        return cell\n",
    "\n",
    "    def _weight_and_bias(self, in_size, out_size):\n",
    "        \"\"\"\n",
    "        Initializes weights and biases.\n",
    "        \"\"\"\n",
    "        weight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.01))\n",
    "        bias = tf.Variable(tf.constant(1., shape=[out_size]))\n",
    "\n",
    "        return weight, bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, batch_size, checkpoints_path, dropout):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoints = checkpoints_path\n",
    "        self.path_to_graph = checkpoints_path + 'seq2seq'\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def train(self, model, train_data, train_size, num_steps, num_epochs, min_loss=0.3):\n",
    "        \"\"\"\n",
    "        Trains a given model architecture with given train data.\n",
    "        \"\"\"\n",
    "        tf.set_random_seed(1234)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            total_loss = []\n",
    "            timings = []\n",
    "            steps_per_epoch = int(train_size / self.batch_size)\n",
    "            num_epoch = 1\n",
    "            \n",
    "            for step in range(1, num_steps):\n",
    "                beg_t = timeit.default_timer()\n",
    "                X, L = next(train_data)\n",
    "                seq_len = np.max(L)\n",
    "\n",
    "                # For anomaly detection problem we reconstruct input data, so\n",
    "                # targets and inputs are identical.\n",
    "                feed_dict = {\n",
    "                    model.inputs: X,\n",
    "                    model.targets: X,\n",
    "                    model.lengths: L,\n",
    "                    model.dropout: self.dropout,\n",
    "                    model.batch_size: self.batch_size,\n",
    "                    model.max_seq_len: seq_len}\n",
    "                \n",
    "                fetches = [model.loss, model.decoder_outputs, model.train_optimizer]\n",
    "                step_loss, _, _ = sess.run(fetches, feed_dict)\n",
    "\n",
    "                total_loss.append(step_loss)\n",
    "                timings.append(timeit.default_timer() - beg_t)\n",
    "\n",
    "                if step % steps_per_epoch == 0:\n",
    "                    num_epoch += 1\n",
    "\n",
    "                if step % 20 == 0 or step == 1:\n",
    "                    print_progress(\n",
    "                        int(step / 200),\n",
    "                        num_epoch,\n",
    "                        np.mean(total_loss),\n",
    "                        np.mean(step_loss),\n",
    "                        np.sum(timings))\n",
    "                    timings = []\n",
    "\n",
    "                if step == 1:\n",
    "                    _ = tf.train.export_meta_graph(filename=self.path_to_graph + '.meta')\n",
    "                \n",
    "                if np.mean(total_loss) < min_loss or num_epoch > num_epochs:\n",
    "                    model.saver.save(sess, self.path_to_graph, global_step=step)\n",
    "                    print(\"Training is finished.\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(params)\n",
    "t = Trainer(params[\"batch_size\"], params[\"checkpoints\"], params[\"dropout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (epoch 1), average_train_loss = 4.64622, step_loss = 4.64622, time_per_step = 11.393\n",
      "Step 0 (epoch 1), average_train_loss = 4.15088, step_loss = 3.45143, time_per_step = 177.237\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10 ** 6\n",
    "num_epochs = 60\n",
    "\n",
    "train_gen = d.train_generator(params[\"batch_size\"], num_epochs)\n",
    "train_size = d.train_size\n",
    "\n",
    "t.train(model, train_gen, train_size, num_steps, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, the threshold setting is introduced. *Set_threshold* calculates the threshold value using *mean* and *std* of loss values of valid samples. \n",
    "\n",
    "At the testing stage, the model receives benign and anomalous samples.\n",
    "For each sample, the value of loss is calculated. If this value is greater than the threshold, then the request is considered anomalous.\n",
    "\n",
    "If you want to use special checkpoints without training a model, you can import a model from *params[\"checkpoint\"]* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor():\n",
    "    def __init__(self, checkpoints_path, std_factor, vocab):\n",
    "\n",
    "        self.threshold = 0.\n",
    "        self.checkpoints = checkpoints_path\n",
    "        self.path_to_graph = checkpoints_path + 'seq2seq'\n",
    "        self.std_factor = std_factor\n",
    "        self.vocab = vocab\n",
    "        self.__load()\n",
    "\n",
    "    def __load(self):\n",
    "        \"\"\"\n",
    "        Loads model from the checkpoint directory and sets models params. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            loaded_graph = tf.Graph()\n",
    "            with loaded_graph.as_default():\n",
    "                saver = tf.train.import_meta_graph(\n",
    "                    self.path_to_graph + '.meta')\n",
    "\n",
    "            self.sess = tf.Session(graph=loaded_graph)\n",
    "            saver.restore(self.sess, tf.train.latest_checkpoint(\n",
    "                self.checkpoints))\n",
    "\n",
    "            # loading model parameters\n",
    "            self.inputs = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "            self.targets = loaded_graph.get_tensor_by_name('targets:0')\n",
    "            self.lengths = loaded_graph.get_tensor_by_name('lengths:0')\n",
    "            self.dropout = loaded_graph.get_tensor_by_name('dropout:0')\n",
    "            self.batch_size_tensor = loaded_graph.get_tensor_by_name('batch_size:0')\n",
    "            self.seq_len_tensor = loaded_graph.get_tensor_by_name('max_seq_len:0')\n",
    "            self.get_batch_loss = loaded_graph.get_tensor_by_name('batch_loss:0')\n",
    "            self.get_probabilities = loaded_graph.get_tensor_by_name('probs:0')\n",
    "            self.get_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError('Unable to create model: {}'.format(e))\n",
    "\n",
    "    def set_threshold(self, data_gen):\n",
    "        \"\"\"\n",
    "        Calculates threshold for anomaly detection.\n",
    "        \"\"\"\n",
    "        \n",
    "        total_loss = []\n",
    "        for seq, l in data_gen:\n",
    "            batch_loss, _ = self._predict_for_request(seq, l)\n",
    "            total_loss.extend(batch_loss)\n",
    "\n",
    "        mean = np.mean(total_loss)\n",
    "        std = np.std(total_loss)\n",
    "        self.threshold = mean + self.std_factor * std\n",
    "\n",
    "        print('Validation loss mean: ', mean)\n",
    "        print('Validation loss std: ', std)\n",
    "        print('Threshold for anomaly detection: ', self.threshold)\n",
    "        \n",
    "        return self.threshold\n",
    "\n",
    "    def predict(self, data_gen, visual=True):\n",
    "        \"\"\"\n",
    "        Predicts probabilities and loss for given sequences.\n",
    "        \"\"\"\n",
    "        loss = []\n",
    "        predictions = []\n",
    "        num_displayed = 0\n",
    "        \n",
    "        for seq, l in data_gen:\n",
    "            batch_loss, alphas = self._predict_for_request(seq, l)\n",
    "            loss.extend(batch_loss)\n",
    "            alphas = self._process_alphas(seq, alphas, 1)\n",
    "            mask = np.array([l > self.threshold for l in batch_loss])\n",
    "            final_pred = mask.astype(int)\n",
    "            predictions.extend(final_pred)\n",
    "            \n",
    "            if visual and num_displayed < 10 and final_pred == [1]:\n",
    "                print('\\n\\nPrediction: ', final_pred[0])\n",
    "                print('Loss ', batch_loss[0])\n",
    "                \n",
    "                num_displayed += 1 \n",
    "                self._visual(alphas, seq)\n",
    "        \n",
    "        return predictions, loss\n",
    "\n",
    "    def _predict_for_request(self, X, l):\n",
    "        \"\"\"\n",
    "        Predicts probabilities and loss for given data. \n",
    "        \"\"\"\n",
    "        lengths = [l]\n",
    "        max_seq_len = l\n",
    "        feed_dict = {\n",
    "            self.inputs: X,\n",
    "            self.targets: X,\n",
    "            self.lengths: lengths,\n",
    "            self.dropout: 1.0,\n",
    "            self.batch_size_tensor: 1,\n",
    "            self.seq_len_tensor: max_seq_len}\n",
    "\n",
    "        fetches = [self.get_batch_loss, self.get_probabilities]\n",
    "        batch_loss, alphas = self.sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "        return batch_loss, alphas\n",
    "\n",
    "    def _process_alphas(self, X, alphas, batch_size):\n",
    "        \"\"\"\n",
    "        Counts numbers as probabilities for given data sample.\n",
    "        \"\"\"\n",
    "        processed_alphas = []\n",
    "        for i in range(batch_size):\n",
    "            probs = alphas[i]\n",
    "            coefs = np.array([probs[j][X[i][j]] for j in range(len(X[i]))])\n",
    "            coefs = coefs / coefs.max()\n",
    "            processed_alphas.append(coefs)\n",
    "            \n",
    "        return processed_alphas\n",
    "\n",
    "    def _visual(self, alphas, X):\n",
    "        \"\"\"\n",
    "        Colors sequence of malicious characters.\n",
    "        \"\"\"\n",
    "        for i, x in enumerate(X):\n",
    "            coefs = alphas[i]\n",
    "            tokens = self.vocab.int_to_string(x)\n",
    "            \n",
    "            for j in range(len(x)):\n",
    "                token = tokens[j]\n",
    "                if coefs[j] < 0.09:\n",
    "                    c = Fore.GREEN\n",
    "                else:\n",
    "                    c = Fore.BLACK\n",
    "                if token != '<PAD>' and token != '<EOS>':\n",
    "                    token = ''.join(c + token)\n",
    "                    print(token, end='')\n",
    "                    \n",
    "            print(Fore.BLACK + '', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/seq2seq-894\n"
     ]
    }
   ],
   "source": [
    "p = Predictor(params[\"checkpoints\"], params[\"std_factor\"], params[\"vocab\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = d.val_generator()\n",
    "threshold = p.set_threshold(val_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benign samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here FP samples are showed and FP rate is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = d.test_generator()\n",
    "valid_preds, valid_loss = p.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of FP: ', np.sum(valid_preds))\n",
    "print('Number of samples: ', len(valid_preds))\n",
    "print('FP rate: {:.4f}'.format(np.sum(valid_preds) / len(valid_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomalous samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here TP samples are showed and TP rate is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = Data(path_anomaly_data, predict=True)\n",
    "pred_gen = pred_data.predict_generator()\n",
    "anomaly_preds, anomaly_loss = p.predict(pred_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of TP: ', np.sum(anomaly_preds))\n",
    "print('Number of samples: ', len(anomaly_preds))\n",
    "print('TP rate: {:.4f}'.format(np.sum(anomaly_preds) / len(anomaly_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the results, let's compute metrics of quality: precision, recall, ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.concatenate(([0] * len(valid_preds), [1] * len(anomaly_preds)), axis=0)\n",
    "preds = np.concatenate((valid_preds, anomaly_preds), axis=0)\n",
    "loss_pred = np.concatenate((valid_loss, anomaly_loss), axis=0)\n",
    "assert len(y_true)==len(loss_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_true, preds)\n",
    "recall = recall_score(y_true, preds)\n",
    "print('Precision: {:.4f}'.format(precision))\n",
    "print('Recall: {:.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_true, loss_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.4f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
